# References of Decentralized-Stochastic-Optimization 
A references list of papers dedicated to decentralized stochastic optimization

# Contents

 - [Angelia Nedic](#angelia-nedic)
 - [Ali Sayed](#ali-sayed)
 - [Mingyi Hong](#mingyi-hong)
 - [Michael Rabbat](#michael-rabbat)
 - [Guanghui Lan](#guanghui-lan)
 - [Ji Liu](#ji-liu)
 - [Martin Jaggi](#martin-jaggi)
 - [Yuejie Chi](#yuejie-chi)
 - [Other authors](#other-authors)
 

## Angelia Nedic

* Distributed Subgradient Methods and Quantization Effects, 2008, https://arxiv.org/abs/0803.1202

* Distributed Stochastic Subgradient Projection Algorithms for Convex Optimization, 2010,
https://arxiv.org/abs/0811.2595

* Asynchronous Gossip Algorithm for Stochastic Optimization: Constant Stepsize Analysis, 2010, https://link.springer.com/chapter/10.1007%2F978-3-642-12598-0_5

* Asynchronous Stochastic Convex Optimization over Random Networks: Error Bounds, 2010, http://ieeexplore.ieee.org/document/5454103/

* Distributed Asynchronous Constrained Stochastic Optimization, 2011, https://ieeexplore.ieee.org/document/5719290

* A distributed adaptive steplength stochastic approximation method for monotone stochastic Nash Games, 2013, https://arxiv.org/abs/1303.4476

* On Stochastic Subgradient Mirror-Descent Algorithm with Weighted Averaging, 2013, https://arxiv.org/abs/1307.1879

* Stochastic Gradient-Push for Strongly Convex Functions on Time-Varying Directed Graphs, 2014, https://arxiv.org/abs/1406.2075

* Dynamic stochastic optimization, 2015, http://ieeexplore.ieee.org/document/7039377/

* Convergence Rate of Distributed Averaging Dynamics and Optimization in Networks, 2015, https://www.researchgate.net/publication/279284232_Convergence_Rate_of_Distributed_Averaging_Dynamics_and_Optimization_in_Networks

* Stochastic quasi-Newton methods for non-strongly convex problems:convergence and rate analysis, 2016, https://arxiv.org/abs/1603.04547

* On Projected Stochastic Gradient Descent Algorithm with Weighted Averaging for Least Squares Regression, 2016, https://arxiv.org/abs/1606.03000

* On stochastic and deterministic quasi-Newton methods for non-Strongly convex optimization: Asymptotic convergence and rate analysis, 2017, https://arxiv.org/abs/1710.05509

* SUCAG: Stochastic Unbiased Curvature-aided Gradient Method for Distributed Optimization, 2018, https://arxiv.org/abs/1803.08198

* A Distributed Stochastic Gradient Tracking Method, 2018, https://arxiv.org/abs/1805.11454

* (review) Network Topology and Communication-Computation Tradeoffs in Decentralized Optimization, 2018, https://arxiv.org/abs/1709.08765

## Ali Sayed

* Decentralized Resource Assignment in Cognitive Networks Based on Swarming Mechanisms Over Random Graphs, 2012, https://ieeexplore.ieee.org/document/6176250

* Stochastic gradient descent with finite samples sizes, 2016, https://ieeexplore.ieee.org/document/7738878

* Diffusion stochastic optimization with non-smooth regularizers, 2016, https://ieeexplore.ieee.org/document/7472458

* On the performance of random reshuffling in stochastic learning, 2017, http://ita.ucsd.edu/workshop/17/files/paper/paper_3133.pdf

* Distributed Coupled Multi-Agent Stochastic Optimization, 2017, https://arxiv.org/abs/1712.08817

* Performance Limits of Stochastic Sub-Gradient Learning, Part II: Multi-Agent Case, 2017, https://arxiv.org/abs/1704.06025

* Stochastic Learning under Random Reshuffling with Constant Step-sizes, 2018, https://arxiv.org/abs/1803.07964

* Variance-Reduced Stochastic Learning by Networked Agents Under Random Reshuffling, 2019,
https://arxiv.org/abs/1708.01384

* COVER: A Cluster-based Variance Reduced Method for Online Learning, 2019, https://ieeexplore.ieee.org/document/8682527

## Mingyi Hong

* Stochastic Proximal Gradient Consensus Over Random Networks, 2015, https://arxiv.org/abs/1511.08905

* GNSD: A gradient-tracking based nonconvex stochastic algorithm for decentralized optimization, 2019, 

## Michael Rabbat

* Multi-agent mirror descent for decentralized stochastic optimization, 2015,
https://ieeexplore.ieee.org/document/7383850/

* Stochastic Gradient Push for Distributed Deep Learning, 2018, https://arxiv.org/abs/1811.10792

## Guanghui Lan

* Communication-efficient algorithms for decentralized and stochastic optimization, 2017,
https://arxiv.org/abs/1701.03961

* Asynchronous decentralized accelerated stochastic gradient descent, 2018, 
https://arxiv.org/abs/1809.09258

## Ji Liu

* Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent, 2017,
https://arxiv.org/abs/1705.09056

* Asynchronous Decentralized Parallel Stochastic Gradient Descent. 2017,
https://arxiv.org/abs/1710.06952

* D<sup>2</sup>: Decentralized Training over Decentralized Data, 2018,
https://arxiv.org/abs/1803.07068

## Martin Jaggi

* Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication, 2019,
https://arxiv.org/abs/1902.00340

* A Unified Theory of Decentralized SGD with Changing Topology and Local Updates, 2020,
https://arxiv.org/abs/2003.10422

## Yuejie Chi

* Convergence of Distributed Stochastic Variance Reduced Methods without Sampling Extra Data, 2019,  https://arxiv.org/pdf/1905.12648.pdf

* Communication-Efficient Distributed Optimization in Networks with Gradient Tracking and Variance Reduction, 2019, https://arxiv.org/pdf/1909.05844.pdf

## Other Authors

* Dual Averaging for Distributed Optimization: Convergence Analysis and Network Scaling, 2011,
https://arxiv.org/abs/1005.2012

* DSA: Decentralized Double Stochastic Averaging Gradient Algorithm, 2016,
https://arxiv.org/abs/1506.04216

* Proximity Without Consensus in Online Multiagent Optimization, 2016,
https://arxiv.org/abs/1606.05578

* Asynchronous Accelerated Proximal Stochastic Gradient for Strongly Convex Distributed Finite Sums, 2019,
https://arxiv.org/abs/1901.09865

* Hop: Heterogeneity-Aware Decentralized Training1606.05578, 2019,
https://arxiv.org/abs/1902.01064

* Towards Byzantine-resilient Learning in Decentralized Systems, 2020,
https://arxiv.org/abs/2002.08569

* Decentralized gradient methods: does topology matter?, 2020
https://arxiv.org/abs/2002.12688

**[â¬† Return to top](#contents)**
